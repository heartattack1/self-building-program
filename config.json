{
  "llm": {
    "mode": "stub",
    "timeout_ms": 30000,
    "max_output_chars": 16000,
    "seed": null,
    "temperature": 0.2,
    "max_tokens": 512,
    "top_p": 0.9,
    "inference4j": {
      "model_path": "./models/llama3.gguf",
      "context_length": null
    }
  }
}
